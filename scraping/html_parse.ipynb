{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from BeautifulSoup import BeautifulSoup\n",
    "import re, string\n",
    "import json\n",
    "\n",
    "def parse_json_stream(stream):\n",
    "    decoder = json.JSONDecoder()\n",
    "    while stream:\n",
    "        obj, idx = decoder.raw_decode(stream)\n",
    "        yield obj\n",
    "        stream = stream[idx:].lstrip()\n",
    "\n",
    "with open('linkedin_html.json', 'r') as f:\n",
    "    read_data = parse_json_stream(f.readlines()[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "def parse_datetime(datestring):\n",
    "    x = sorted(datestring.split())\n",
    "    if not x:\n",
    "        return ''\n",
    "    if len(x) == 1:\n",
    "        return str(datetime.strptime(x[0], '%Y'))\n",
    "    return str(datetime.strptime(' '.join(x), '%Y %B'))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def clean_str(text):\n",
    "    return ''.join([i if ord(i) < 128 else ' ' for i in text])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def return_date(html):\n",
    "    dates = html.findAll('time')\n",
    "    dates = [x.string for x in dates]\n",
    "    dates = [clean_str(x) for x in dates]\n",
    "    dates = [parse_datetime(x) for x in dates]\n",
    "    \n",
    "    datestr = html.find('span',{'class':'experience-date-locale'})\n",
    "    if datestr and len(dates) == 1:\n",
    "        if 'present' in datestr.text.lower():\n",
    "            dates.append(str(datetime.today()))\n",
    "    \n",
    "    return tuple(dates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def find_ed(soup):\n",
    "    education = soup.findAll('div', {'class':re.compile(\"(^education)\")})\n",
    "    all_ed = []\n",
    "    for item in education:\n",
    "        ed_dict = {}\n",
    "        head = item.find('header')\n",
    "        \n",
    "        ed_dict['institution'] = ''.join(head.h4.findAll(text=True))        \n",
    "        ed_dict['degree'] = ''.join(head.h5.findAll(text=True))\n",
    "        ed_dict['dates'] = return_date(item)\n",
    "\n",
    "        all_ed.append(ed_dict)\n",
    "    return all_ed\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def find_exp(soup):\n",
    "    experiences = soup.findAll('div', {'id':re.compile(\"(^experience.*-view)\")})\n",
    "    all_exp = []\n",
    "    \n",
    "    for item in experiences:\n",
    "        exp_dict = {}\n",
    "        exp_dict['title'] = item.find('a', {'title':'Learn more about this title'}).string\n",
    "        exp_dict['organization'] = item.find('h5', attrs={'class': None}).next.string\n",
    "        exp_dict['dates'] = return_date(item)\n",
    "        \n",
    "        if item.find('span', {'class': 'locality'}):\n",
    "            exp_dict['locality'] = item.find('span', {'class': 'locality'}).string   \n",
    "\n",
    "        all_exp.append(exp_dict)\n",
    "    return all_exp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "422"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "linkedin_data = {}\n",
    "\n",
    "for idx, profile in enumerate(read_data):    \n",
    "    \n",
    "    sys.stdout.write(\"\\r%d\" % idx)\n",
    "    sys.stdout.flush()\n",
    "    \n",
    "    name, html = profile[:]\n",
    "    soup = BeautifulSoup(html)\n",
    "    person = {}\n",
    "    person['edu'] = find_ed(soup)\n",
    "    person['exp'] = find_exp(soup)\n",
    "    link = soup.findAll('link', {'rel':'canonical'})\n",
    "    if len(link)>0:\n",
    "        person['link'] = link[0]['href']\n",
    "    linkedin_data[name] = person"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open('linkedin_data2.json','w') as f:\n",
    "    json.dump(linkedin_data, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
